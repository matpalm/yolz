{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc272dc-fa4f-4ffe-8c5e-18b5964ceb73",
   "metadata": {},
   "source": [
    "# v1 embedding model\n",
    "\n",
    "* v1 reproducing constrastive loss for metric learning; (2N, H, W, 3)\n",
    "* v1.5 vectorising that model (B, 2N, H, W, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79017548-62c2-4e27-85cc-a68fd8a25ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'jax'\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67c87a4-5e25-414f-b3b4-2be157a651f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras 3.6.0 jax 0.4.33 optax 0.2.3\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax import jit, value_and_grad, vmap\n",
    "\n",
    "import optax\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, GlobalMaxPooling2D\n",
    "from keras.layers import Layer, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('keras', keras.__version__, \n",
    "      'jax', jax.__version__, \n",
    "      'optax', optax.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "354e80e4-c9cd-4724-a458-fcb0805f8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opts:\n",
    "    height_width = 64\n",
    "    batch_size = 4          # B (outer batch size, used for v1.5)\n",
    "    num_classes = 6         # C total number of classes\n",
    "    num_egs_per_class = 3   # N number of examples per class\n",
    "    embedding_dim = 64      # E embedding dim\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "opts = Opts()\n",
    "\n",
    "def shapes(debug_str, list_of_variables):\n",
    "    return f\"{debug_str} {[v.shape for v in list_of_variables]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9681243f-0f72-47f7-a55d-3118185ab46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting debug\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def collage(pil_imgs, rows, cols):\n",
    "    n = len(pil_imgs) \n",
    "    if n != rows * cols:\n",
    "        raise Exception()\n",
    "    img_h, img_w = pil_imgs[0].size    \n",
    "    collage = Image.new('RGB', (rows*img_h, cols*img_w))\n",
    "    for i in range(n):\n",
    "        pr, pc = i%rows, i//rows\n",
    "        collage.paste(pil_imgs[i], (pr*img_h, pc*img_w))\n",
    "    return collage\n",
    "    \n",
    "def to_pil_img(a):\n",
    "    return Image.fromarray(np.array(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f22e10-db67-4783-aca1-d4b08a7ed85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.label_idx_to_str {0: '061', 1: '111', 2: '000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 16:19:24.866771: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version 12.6.77. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6, 64, 64, 3) tf.Tensor(\n",
      "[[2 2 1 1 0 0]\n",
      " [1 1 2 2 0 0]\n",
      " [2 2 1 1 0 0]\n",
      " [2 2 0 0 1 1]], shape=(4, 6), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "from data import ConstrastiveExamples\n",
    "\n",
    "# start with simple case of x1 R, G, B example\n",
    "c_egs = ConstrastiveExamples(\n",
    "    root_dir='data/reference_egs',\n",
    "    obj_ids=[\"061\", # \"135\",\"182\",  # x1 red\n",
    "             \"111\", # \"153\",\"198\",  # x1 green\n",
    "             \"000\", # \"017\",\"019\"   # x2 blue\n",
    "            ]\n",
    ")\n",
    "ds = c_egs.dataset(batch_size=opts.batch_size,\n",
    "                   objs_per_batch=opts.num_egs_per_class)\n",
    "for x, y in ds:\n",
    "    break\n",
    "\n",
    "x = jnp.array(x)\n",
    "print(x.shape, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54847fa9-847b-4ec0-a1e3-d631076b08c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAADACAIAAABDBPzwAAANQklEQVR4Ae1dzY8cRxWv7l3vzMbZySLsECl4zSkhEQcgCUIcokiRcgAkhAAhJQoCIZAhAnFA/AHkgIQ4cAmKOIEiDChGBAEBEgmFCHEwOGAOxCQQo40N/iDBu+v1zqx3unjVNV2uqa7ura6uj96d1xr1VNfXe+/3q1df3dOTUEoJHvEQSOOJRskMASQgcjtAApCAyAhEFo8egARERiCyePQAJCAyApHFowcgAZERiCwePQAJiIxAZPHzYeT3l44Tsgg7H8ONjwiJ/cGzdPw3uBxtflVEzlrAOwH9wW8ISXJYt+DcX/pZkh6idI1mFwjdyOPnZg102V6/BCwOfgvCaHYezgmZJ8kcoJ+N/840oG+yM1kcbT6WB2b05JcASq4T+gah1wBddtuBblE6zBv+9Xwjtjfj6AMkHgnoLz1DyHVKxyAmP1gXVDR88Ibe6OoXeMIsn30RwNEHAvJPDv0UzDuE7kxFzOqFv3UA4AvozxPoc/htn2R5VkGus9sfASAV3IvfcM6YCvQKOxfHLE89CwzYtzcCkoUCfTYC41GFgBcC+oNfwISHiaTQC+FRh4AXAobrH66TiWkSAl5mQf3Bc7kIeOQo7/0leV0ILi6/RLN/55rAEv3AcP2hiFolPh7Mgv0GQmDBBYuAC1rbYo3Ai7ecJMl8gb6imnhALR2uf0hJ83fpxQMI6egcHzZFaPbPHM0+nJPkFpIul8DN+oOfs0hYQrKpBGyi9LbWHyxlcxPhjYDuDb/9pZ9S2BeZHLA6IWxfZHyxiIHhcCGZOzq5ZOgf4BM5SkawcVtsKbL0JDm8tXZvUbDVtycCevkqrJVmzgtL6GvrhuFqSPlGIaQnb03IzSxfchOnIT/DmMF2dim93B/8iqWya+ApJQmbzmytPQDnRod7AvqDXxOyDUpQInaBGqnkLTO92qBvpG/SyX4t6APgpkl6G9MsyVlhNDAmkmQpX+pnFHZW6Hp/6UQeT4cbH2eZDQ73BBCBO72sVSDWCNwAfVVvcI6MZuc4E5PEfHig5KDImyRsXOFHf+knw42PFVd13+7XAXT8cu6SrIF06nBBPDAB84v8Q8HLF4veCQzdhC5O9HI0O9s7+C0T890TkKTvyAXDMNC5w5QD1s/ArLT+wxf56kKH0ZCtmlvuowsylx4hJ+dgl+bJRgvzA4a6/K4q8wk4ODGmxWeOAA6M7Aq9g9+2Hh7SubtgRkToVZoB+s2g55o4JqA/+CWvl2b/4oHun0ebX5GV3MU5pKzp3LvYHjtdz+Ns0IeCjrci8nkY06eKALnp5Xp3+lRDxtzCR+n4LHu2A4zV7bgYWurYAzoNZ3PlBIhlJrLxGTp+1brvErogAQKKuoBgAjIBGen8B7KdP0gF+JxbbOc1eNTMJQGLg+clnfZtkJPRO3gyn/yMCjsna2NYsslsFamV3y4JoPkOBIiqGgAqtdiDCcrQbW2B04UYvw1prctMFnTpATmA6spQRrWRb8oF93HYqQdMbkDeGIv2MXCuTHNGQP4UNGiF6DejxhkBMPpPJE+2RJrpMbO5HRIAa3FK6DY8+z+zaFoY7m4QzhB3C/zdPZ5OyWYhf1gEpr5xCjQFR3HhzgMoXxNabgoW+szct8MxgO+HzByCLQ12SABoUtn8sf+p4skZAaPNL1fJQPSrkIF4xzdkSvvm8DO8L9WIxyTHBCCgTRFw1gU1FYz5OQJIQOSWgAQgAZERiCwePQAJiIxAZPHoAUhAZAQii0cPQAIiIxBZPHoAEhAZgcji0QOQgMgIRBaPHoAEREYgsnj0ACQgMgKRxaMHIAGREYgsHj0ACYiMQGTxfj3g9ifYK786eBz53sKtXzsAn+i6eXwuSDbv0jcrn1oMD4GsGEiPq5sXD+hI49JSq6APeW57PKYfuCegbCEYqY3UAhQ+MtsIL/OGRPcE3Kh774Qito9wBEQ0Um4JVT1+LPXcE1BloYxC3HCVhpyDlacW4BNMQ/cEBFO9jaAaDuCNrfBb5yNPLcCnjQjDsr6moVUeXWW5obpus2mV7N0t/dYqfxfc64/yl8G5FT6pzd2P9LyoF7rSKfRBeP7qWVi1wVv5tDSwpPyAlyCfO2bDk68uqKqlaxtdaJgLeYqSKvpFNvY9JoC1gJunHP1xj7/ZEtDffs3yHQ2+CJCV73JYcFCHvmQAcMDbEEM/P0YvU46+XdvyNQZw5ep1EsZLBoYOvv3JBemNwzrpxSsweNrozKSlL97D2u7Wqenk5hsbHj2gHn3QftcMOjwcx011HABm+SMJFOjzuDL6EN/UKGceUCM4HZBsXbJDCnbBCVjPrrZjScUiqKBfRKvfTS2yJ6AGcVmpGvR5tqYay5U7CQMB8IadpHY+6Al90N+UAEO47RCJzsHtTy6k0qqL/WcDvHvn2sQaf+iDgFrem/doE5VNvmD0Scilb3ToPgFozaGX1TdBv00DqhuEvbZ61u125h82zh/bhn/AsEMf2GoDVCUBbSqVW9BeCafwZwzSMXyJmrR9UcIaLk0XZF2X0EYfAK4NJhv6sv5jVx/dXvnRZBwA9P0LnEhQPcAX+iBOh75HcRYQ5uORNfp2tkwRYFeFhaXdLAJO0PIf6CwAnCLAKy5VUwULpf3pOfxruM6HW6EZA5ybVwW9c0EtKzz6dG/hjmT7FUsO7Myc8gC7KqrMTnrskRu3dVbJah+/8oOF8RqDHjiwqM3aTHUl7KRDqNGmqv6aIhZwNC0C6EORbIuVs/CANsqrXRDUVYXRrlZBk7/4eLdWtrvqDN0OvOxXZAuMPsid6oKEHk0DvLcxQb+qsViz3lRVJb9Af+dC6LbPNdEQUIWRojpcwt8pmkNfLt6FmNVH2I1cQH/n4g0/4IoBDvVQ1KcaWqd2QYbF2vQ2oHes9q61bnhahR6yCXCrtBUZtHWaR2o8QBZfrshTk3dlT1nhXWPKopUY5bIen13FKRnUWZBILjfSNq1eVCsCvP6ybSJD4ICwt0olTwobEQC3rS9+fY9NbwLzZy2ukgCoETivag7W8rCggkAdAUpWvPSBgH4Q9iEJ69QigARoYQkXiQSEw1orCQnQwhIuEgkIh7VWEhKghSVcJBIQDmutJCRAC0u4SCQgHNZaSUiAFpZwkUhAOKy1kpAALSzhIpGAcFhrJSEBWljCRSIB4bDWSkICtLCEi0QCwmGtlYQEaGEJF4kEhMNaKwkJ0MISLhIJCIe1VhISoIUlXKTls6HhFAwi6fvpg0My3sl/R/hY9mIQmRMh6AEMCIF+SOi5LCQgPOZTEpGAKTjCXyABDHObn+U54goJcASkbTVIgC1yjsohAY6AtK0GCbBFzlE5JIAB+fnsBUd4Nq4GCWgMmdsCuBWhx/OJ9H6e8B5yiAeukO2zZN35RkUnCDidfvL35D/cTucW6gE2jv0z+a9xXpuMkQkQDc1G931RJtqP9GqgD+kEihp9MncnWZ4n6SlyWcuvc90ieIBis9bOWJGwLXqavAE0BFMgKAFdhp4jfoCk10kGNJQJeD95G0SeSj8hkmAH6b3Z0+LSLhCoC2oKvXNPr0dHqMcJqM8sUt+dT5CEu2Ts3xzIfdkJkcEk4N0DhG0m2sTKA3xzPaH5gw7isl6fv+gmSPfVlymleiTglfTh58m5kkSjCIAjmBOspp+ieeMVmr2aPiLCvgO+CKhv+A+RI8+R133bVl8/1/Aecnguvx3wPnKryH+FjERYCYhmoTVQpCqlai7dE6DVTNEgOvpCHzHdPEqWDpE+xHM+RAYlcCZ9GHZv7siOK1iD1UqMUrDq0uUgrEB/E5mH/0QbkfEHycqzZLVKg5p4O5NqKpSTFG1FEqh9F3mLuAQ+TpJL95LDIoYHlgh70SJMhFKS3JkdV1LNL90QIBvzTrJ8nmwC9Bx9c1XknF6h54K+k96veVWZpITChEjR3r/kfECeu7MfipwmgbYEfDd9gM8cQNgSYX/Mu0HavtopAPqgp9xo6pGqYoKXKvPxJ3LZ3IS2Y4BA3wn0UIm56vWoOUy9RnbEUFEmQ3Yjkc1celsCzCXV5wyPO0g0dwKhvEwGzKBE/D/I2hqx+SvDyASEx11A1j5g0d7LQu0J+F/6We2eSVlGOWZP4142p02MDQEX00/vEPoMOdtIMMznjmW/a1RkFjI3IwCgv0S2XizuXpkA1OXGznWzGAlMDDfMY0oAQH+CvKZUCjMw2D78XPZC2YYu465YEZeG3dcBAD0sBVfJVVlvPvn9ovQoPedgD+EumyOHy41JTjUMm+NQScDl9DOwlP1jCXrQANCXoTfUaW9la0mDOQH6LgjEr5CblVYPCOJAatKMzNGH2vQEQIKCPkJvAj3kaYR+HQGyvKaVymVnJ2yHkt4DoK59M6i6bQECZWWQEPFNxekJgFqsa2yqwR7Nz/EBGloChQ/ntmoALdEH2UhAAwLaw10WVrkOKGedwRjR0fuAnuOJBERuV9gFIQGREYgsHj0ACYiMQGTx6AFIQGQEIotHD0ACIiMQWTx6ABIQGYHI4tEDkIDICEQWjx6ABERGILL4/wPX7B1DoQWuAwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x192>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall each element of outer batch is xN examples of anchor/positive pairs\n",
    "\n",
    "pil_imgs = list(map(to_pil_img, x[0]))\n",
    "collage(pil_imgs, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a4075-6a9f-4214-a97e-49f4d04312bc",
   "metadata": {},
   "source": [
    "model is simple enough embedding model\n",
    "\n",
    "output is L2 normalised embedding ( so dot products can be used for sims and xent contrastive )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e812617-4ac4-43fb-8b7d-e795703f8a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_24          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_26          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_27          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d_6          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embeddings (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ l2_normalisation_6              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L2Normalisation</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_24          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_24 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_25 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_26          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_26 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_27 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_27          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_27 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d_6          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embeddings (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ l2_normalisation_6              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mL2Normalisation\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,592</span> (416.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,592\u001b[0m (416.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,112</span> (414.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m106,112\u001b[0m (414.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">480</span> (1.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m480\u001b[0m (1.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def conv_bn_relu(filters, y):\n",
    "    y = Conv2D(filters=filters, strides=2, kernel_size=3, activation=None, padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    return Activation('relu')(y)\n",
    "\n",
    "class L2Normalisation(Layer):\n",
    "    def call(self, x):\n",
    "        norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n",
    "        return x / norm\n",
    "\n",
    "def construct_embedding_model():\n",
    "    input = Input((opts.height_width, opts.height_width, 3))\n",
    "    y = conv_bn_relu(filters=16, y=input)\n",
    "    y = conv_bn_relu(filters=32, y=y)\n",
    "    y = conv_bn_relu(filters=64, y=y)\n",
    "    y = conv_bn_relu(filters=128, y=y)\n",
    "    # y = Dropout(0.5)(y)\n",
    "    y = GlobalMaxPooling2D()(y)  # (B, E)\n",
    "\n",
    "    # embed, with normalisation\n",
    "    embeddings = Dense(\n",
    "        opts.embedding_dim,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=keras.initializers.TruncatedNormal(),\n",
    "        name='embeddings')(y)  # (B, E)\n",
    "    embeddings = L2Normalisation()(embeddings)\n",
    "    \n",
    "    return Model(input, embeddings)    \n",
    "\n",
    "embedding_model = construct_embedding_model()\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59faf814-9b5a-418d-8054-34252bd4a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = [ntv.name=='seed_generator_state' for ntv in embedding_model.non_trainable_variables]\n",
    "if np.any(seed):\n",
    "    # we need to include code to have vectorised non_trainable_variables being a seed\n",
    "    # simplest might be to just take first? ( since they should all be the same? )\n",
    "    raise Exception(\"TODO!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0a23c25-7179-4c24-b043-2b42b83ed9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e shape (6, 64)\n",
      "e norms [0.99999994 1.         1.         1.         0.9999999  0.99999994]\n",
      "ntps [(16,), (16,), (32,), (32,), (64,), (64,), (128,), (128,)]\n"
     ]
    }
   ],
   "source": [
    "# the model sees \"a batch\" as the set of (anchor, positive) pairs\n",
    "# whereas x is a batch of these.\n",
    "\n",
    "params = embedding_model.trainable_variables\n",
    "nt_params = embedding_model.non_trainable_variables\n",
    "\n",
    "embeddings, nt_params_2 = embedding_model.stateless_call(params, nt_params, x[0], training=True)\n",
    "\n",
    "print(\"e shape\", embeddings.shape)\n",
    "print(\"e norms\", jnp.linalg.norm(embeddings, axis=-1))\n",
    "print(shapes('ntps', nt_params_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72fe66e3-4460-438b-bbfd-abac504015c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 3.2866418\n",
      "ntps [(16,), (16,), (32,), (32,), (64,), (64,), (128,), (128,)]\n"
     ]
    }
   ],
   "source": [
    "# define the constrastive loss based on the 'batch' of 2N examples ( N pairs )\n",
    "\n",
    "def main_diagonal_softmax_cross_entropy(logits):\n",
    "    # cross entropy assuming \"labels\" are just (0, 1, 2, ...) i.e. where\n",
    "    # one_hot mask for log_softmax ends up just being the main diagonal\n",
    "    return -jnp.sum(jnp.diag(jax.nn.log_softmax(logits)))\n",
    "    \n",
    "def constrastive_loss(params, nt_params, x):\n",
    "    embeddings, nt_params = embedding_model.stateless_call(params, nt_params, x, training=True)\n",
    "    embeddings = embeddings.reshape((opts.num_egs_per_class, 2, opts.embedding_dim))\n",
    "    anchors = embeddings[:, 0]\n",
    "    positives = embeddings[:, 1]\n",
    "#    print('anchors', anchors.shape, 'positives', positives.shape)\n",
    "    gram_ish_matrix = jnp.einsum('ae,be->ab', anchors, positives)\n",
    "    xent = main_diagonal_softmax_cross_entropy(logits=gram_ish_matrix)\n",
    "    return jnp.mean(xent), nt_params\n",
    "\n",
    "loss, nt_params_2 = constrastive_loss(params, nt_params, x[0])\n",
    "print('loss', loss)\n",
    "print(shapes('ntps', nt_params_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43b6af92-0972-4097-b134-77f2c030f0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1643927\n",
      "1.1071905\n",
      "1.106949\n",
      "1.1069435\n",
      "1.1069436\n",
      "1.1069434\n",
      "1.1069434\n",
      "1.1069435\n",
      "1.1069434\n",
      "1.1069436\n"
     ]
    }
   ],
   "source": [
    "# define gradients and a simple training loop\n",
    "\n",
    "def calculate_gradients(params, nt_params, x):\n",
    "    # x (2C,H,W,3)\n",
    "    grad_fn = value_and_grad(constrastive_loss, has_aux=True)    \n",
    "    (loss, nt_params), grads = grad_fn(params, nt_params, x)\n",
    "    return (loss, nt_params), grads\n",
    "\n",
    "opt = optax.adam(learning_rate=opts.learning_rate)\n",
    "\n",
    "def train_step(params, nt_params, opt_state, x):\n",
    "    (loss, nt_params), grads = calculate_gradients(params, nt_params, x)\n",
    "    updates, opt_state = opt.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, nt_params, opt_state, loss\n",
    "\n",
    "embedding_model = construct_embedding_model()\n",
    "\n",
    "params = embedding_model.trainable_variables\n",
    "nt_params = embedding_model.non_trainable_variables\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "for e in range(1000):\n",
    "    params, nt_params, opt_state, loss = jit(train_step)(params, nt_params, opt_state, x[0])\n",
    "    if e % 100 == 0:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cea3b7c9-8bbe-4c88-9c23-b001f4561e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1. ,  1. , -0.5, -0.5, -0.5, -0.5],\n",
       "       [ 1. ,  1. , -0.5, -0.5, -0.5, -0.5],\n",
       "       [-0.5, -0.5,  1. ,  1. , -0.5, -0.5],\n",
       "       [-0.5, -0.5,  1. ,  1. , -0.5, -0.5],\n",
       "       [-0.5, -0.5, -0.5, -0.5,  1. ,  1. ],\n",
       "       [-0.5, -0.5, -0.5, -0.5,  1. ,  1. ]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test against batch\n",
    "embeddings, _ = embedding_model.stateless_call(params, nt_params, x[0], training=False)\n",
    "embeddings.shape\n",
    "\n",
    "# looks good (0,1) (2,3) (4,5) all pair well ( and others are -0.5 )\n",
    "jnp.around(jnp.dot(embeddings, embeddings.T), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a27837-0934-4aad-b986-8c94fc3c2b31",
   "metadata": {},
   "source": [
    "next we get things working on a batch of these examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09f3d449-ffae-4ce6-918f-62c30c15a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e shape (4, 6, 64)\n",
      "e norms [[1.         1.         1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.        ]\n",
      " [1.         1.         1.0000001  1.         1.         1.        ]\n",
      " [1.         1.         0.99999994 1.         0.99999994 1.        ]]\n",
      "ntps_2 [(4, 16), (4, 16), (4, 32), (4, 32), (4, 64), (4, 64), (4, 128), (4, 128)]\n"
     ]
    }
   ],
   "source": [
    "# to use the model batched we vmap it first \n",
    "\n",
    "def training_call(x):\n",
    "    return embedding_model.stateless_call(params, nt_params, x, training=True)\n",
    "\n",
    "training_call = vmap(training_call)\n",
    "embeddings, nt_params_2 = training_call(x)\n",
    "\n",
    "print(\"e shape\", embeddings.shape)\n",
    "print(\"e norms\", jnp.linalg.norm(embeddings, axis=-1))\n",
    "print(shapes('ntps_2', nt_params_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ddc14ef6-32bd-4cea-9299-e5b91b6634ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntps [(16,), (16,), (32,), (32,), (64,), (64,), (128,), (128,)]\n"
     ]
    }
   ],
   "source": [
    "# but note that the nt_params returned have been vectorised too\n",
    "# i.e. they are [(B, p1), (B, p2), ...] instead of [(p1,), (p2,), ...]\n",
    "# so, we need to aggreate them,\n",
    "\n",
    "nt_params_2 = [jnp.mean(p, axis=0) for p in nt_params_2]\n",
    "print(shapes('ntps', nt_params_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02de11b1-7182-407c-a6e4-e6c5f8e0c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before inner batch aggregation...\n",
      "loss_v [1.1069434 1.7856638 1.8490474 1.6445789]\n",
      "ntps [(4, 16), (4, 16), (4, 32), (4, 32), (4, 64), (4, 64), (4, 128), (4, 128)]\n",
      "after inner batch aggregation...\n",
      "loss 1.5965583\n",
      "ntps [(16,), (16,), (32,), (32,), (64,), (64,), (128,), (128,)]\n"
     ]
    }
   ],
   "source": [
    "# as before we can vectorise the loss \n",
    "# takes (B, 2C, H, W, 3)\n",
    "constrastive_loss_v = vmap(constrastive_loss, in_axes=[None, None, 0])\n",
    "\n",
    "# and run over all of x \n",
    "loss_v, nt_params_2 = constrastive_loss_v(params, nt_params, x)  # (N)\n",
    "\n",
    "print(\"before inner batch aggregation...\")\n",
    "print('loss_v', loss_v)\n",
    "print(shapes('ntps', nt_params_2))\n",
    "\n",
    "loss = jnp.mean(loss_v)\n",
    "nt_params_2 = [jnp.mean(p, axis=0) for p in nt_params_2]\n",
    "\n",
    "print(\"after inner batch aggregation...\")\n",
    "print('loss', loss)\n",
    "print(shapes('ntps', nt_params_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "878cbfc8-7ebd-4dbb-beb8-92ab8b7031fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.5965827\n",
      "grads [(3, 3, 3, 16), (16,), (16,), (16,), (3, 3, 16, 32), (32,), (32,), (32,), (3, 3, 32, 64), (64,), (64,), (64,), (3, 3, 64, 128), (128,), (128,), (128,), (128, 64)]\n",
      "ntps [(16,), (16,), (32,), (32,), (64,), (64,), (128,), (128,)]\n"
     ]
    }
   ],
   "source": [
    "# we can calculate grads just as before but we must call a function that includes the loss \n",
    "# aggregation ( i.e. grads only are applicable for a scalar loss ) so wrap the vmap\n",
    "# and jnp.mean in one function.\n",
    "\n",
    "def constrastive_loss_v(params, nt_params, x):\n",
    "    # vectorise function as normal\n",
    "    loss_fn_v = vmap(constrastive_loss, in_axes=[None, None, 0])\n",
    "    # call returning vectorised result\n",
    "    loss_v, nt_params_v = loss_fn_v(params, nt_params, x)\n",
    "    # aggregate mean over both loss and nt_params for return\n",
    "    # TODO: what does this do for rng seeds?\n",
    "    loss = jnp.mean(loss_v)\n",
    "    nt_params = [jnp.mean(p, axis=0) for p in nt_params_v]    \n",
    "    return loss, nt_params\n",
    "\n",
    "(loss, nt_params_2), grads = jit(value_and_grad(constrastive_loss_v, has_aux=True))(params, nt_params, x)\n",
    "\n",
    "print('loss', loss)\n",
    "print(shapes('grads', grads))\n",
    "print(shapes('ntps', nt_params_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88b91293-e34f-4341-812f-3d2368fd0640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 loss 3.2676353\n",
      "e 20 loss 2.000111\n",
      "e 40 loss 1.2002019\n",
      "e 60 loss 1.1234568\n",
      "e 80 loss 1.11344\n",
      "e 100 loss 1.1101773\n",
      "e 120 loss 1.1088454\n",
      "e 140 loss 1.1081421\n",
      "e 160 loss 1.1077328\n",
      "e 180 loss 1.1074806\n"
     ]
    }
   ],
   "source": [
    "# stitch together into a training loop\n",
    "\n",
    "def main_diagonal_softmax_cross_entropy(logits):\n",
    "    # cross entropy assuming \"labels\" are just (0, 1, 2, ...) i.e. where\n",
    "    # one_hot mask for log_softmax ends up just being the main diagonal\n",
    "    return -jnp.sum(jnp.diag(jax.nn.log_softmax(logits)))\n",
    "    \n",
    "def constrastive_loss(params, nt_params, x):\n",
    "    # x (2C,H,W,3)\n",
    "    embeddings, nt_params = embedding_model.stateless_call(params, nt_params, x, training=True)\n",
    "    embeddings = embeddings.reshape((opts.num_egs_per_class, 2, opts.embedding_dim))\n",
    "    anchors = embeddings[:, 0]\n",
    "    positives = embeddings[:, 1]\n",
    "    gram_ish_matrix = jnp.einsum('ae,be->ab', anchors, positives)\n",
    "    xent = main_diagonal_softmax_cross_entropy(logits=gram_ish_matrix)\n",
    "    return jnp.mean(xent), nt_params\n",
    "\n",
    "def constrastive_loss_v(params, nt_params, x):\n",
    "    # x (B,2C,H,W,3)\n",
    "    loss_fn_v = vmap(constrastive_loss, in_axes=[None, None, 0])\n",
    "    loss_v, nt_params_v = loss_fn_v(params, nt_params, x)\n",
    "    loss = jnp.mean(loss_v)\n",
    "    return loss, nt_params_v\n",
    "\n",
    "def calculate_gradients(params, nt_params, x):\n",
    "    # x (B,2C,H,W,3)\n",
    "    grad_fn = value_and_grad(constrastive_loss_v, has_aux=True)    \n",
    "    (loss, nt_params_v), grads = grad_fn(params, nt_params, x)\n",
    "    return (loss, nt_params_v), grads\n",
    "\n",
    "opt = optax.adam(learning_rate=opts.learning_rate)\n",
    "\n",
    "def train_step(params, nt_params, opt_state, x):\n",
    "    (loss, nt_params_v), grads = calculate_gradients(params, nt_params, x)\n",
    "    updates, opt_state = opt.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    nt_params = [jnp.mean(p, axis=0) for p in nt_params_v]\n",
    "    return params, nt_params, opt_state, loss\n",
    "\n",
    "embedding_model = construct_embedding_model()\n",
    "\n",
    "params = embedding_model.trainable_variables\n",
    "nt_params = embedding_model.non_trainable_variables\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "for epoch in range(200):\n",
    "    params, nt_params, opt_state, loss = jit(train_step)(params, nt_params, opt_state, x)\n",
    "    if epoch % 20 == 0:\n",
    "        print('e', epoch, 'loss', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4611475-2611-427d-8661-41eb7dbef8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 0\n",
      "[[ 1.          0.96999997 -0.28       -0.37       -0.59       -0.61      ]\n",
      " [ 0.96999997  1.         -0.26999998 -0.35999998 -0.59999996 -0.61      ]\n",
      " [-0.28       -0.26999998  1.          0.97999996 -0.37       -0.38      ]\n",
      " [-0.37       -0.35999998  0.97999996  1.         -0.29999998 -0.29999998]\n",
      " [-0.59       -0.59999996 -0.37       -0.29999998  1.          0.96      ]\n",
      " [-0.61       -0.61       -0.38       -0.29999998  0.96        1.        ]]\n",
      "---------- 1\n",
      "[[ 1.          0.96999997 -0.28       -0.37       -0.59       -0.61      ]\n",
      " [ 0.96999997  1.         -0.26999998 -0.35999998 -0.59999996 -0.61      ]\n",
      " [-0.28       -0.26999998  1.          0.97999996 -0.37       -0.38      ]\n",
      " [-0.37       -0.35999998  0.97999996  1.         -0.29999998 -0.29999998]\n",
      " [-0.59       -0.59999996 -0.37       -0.29999998  1.          0.96      ]\n",
      " [-0.61       -0.61       -0.38       -0.29999998  0.96        1.        ]]\n",
      "---------- 2\n",
      "[[ 1.          0.96999997 -0.28       -0.37       -0.59       -0.61      ]\n",
      " [ 0.96999997  1.         -0.26999998 -0.35999998 -0.59999996 -0.61      ]\n",
      " [-0.28       -0.26999998  1.          0.97999996 -0.37       -0.38      ]\n",
      " [-0.37       -0.35999998  0.97999996  1.         -0.29999998 -0.29999998]\n",
      " [-0.59       -0.59999996 -0.37       -0.29999998  1.          0.96      ]\n",
      " [-0.61       -0.61       -0.38       -0.29999998  0.96        1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# try a couple of the examples from the batch\n",
    "# each looks good\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"-\"*10, i)\n",
    "    embeddings, _ = embedding_model.stateless_call(params, nt_params, x[1], training=False)\n",
    "    print(jnp.around(jnp.dot(embeddings, embeddings.T), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1206be60-f6e4-45fe-9302-7afce153c5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how do some example look across batches though?\n",
    "\n",
    "x_ = jnp.stack([\n",
    "    x[0][0], \n",
    "    x[0][1],\n",
    "    x[1][4],\n",
    "    x[1][5]\n",
    "])\n",
    "\n",
    "pil_imgs = list(map(to_pil_img, x_))\n",
    "collage(pil_imgs, 2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0860d5ef-e506-4985-8bd9-003c9a14590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.    0.96 -0.01 -0.08]\n",
      " [ 0.96  1.    0.05 -0.04]\n",
      " [-0.01  0.05  1.    0.9 ]\n",
      " [-0.08 -0.04  0.9   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# pretty good! the off diagonals are > 0.9\n",
    "# so it's generalising across the batch\n",
    "embeddings, _ = embedding_model.stateless_call(params, nt_params, x_, training=False)\n",
    "print(jnp.around(jnp.dot(embeddings, embeddings.T), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbcb8a26-22b8-473d-ae03-77fc1f8cf6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.38920677, dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import random\n",
    "key = jnp.array([111440815,         2], dtype=np.uint32)\n",
    "random.uniform(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffe6c2-9e99-47df-8b50-53aba1e7de0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
