{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc272dc-fa4f-4ffe-8c5e-18b5964ceb73",
   "metadata": {},
   "source": [
    "# v1 embedding model\n",
    "\n",
    "* v1 reproducing constrastive loss for metric learning; (2N, H, W, 3)\n",
    "* v1.5 vectorising that model (B, 2N, H, W, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79017548-62c2-4e27-85cc-a68fd8a25ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'jax'\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67c87a4-5e25-414f-b3b4-2be157a651f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras 3.5.0 jax 0.4.33 optax 0.2.3\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax import jit, value_and_grad, vmap\n",
    "\n",
    "import optax\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, GlobalMaxPooling2D\n",
    "from keras.layers import Layer, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('keras', keras.__version__, \n",
    "      'jax', jax.__version__, \n",
    "      'optax', optax.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "354e80e4-c9cd-4724-a458-fcb0805f8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opts:\n",
    "    height_width = 64\n",
    "    batch_size = 4          # B (outer batch size, used for v1.5)\n",
    "    num_classes = 6         # C total number of classes\n",
    "    num_egs_per_class = 3   # N number of examples per class\n",
    "    embedding_dim = 64      # E embedding dim\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "opts = Opts()\n",
    "\n",
    "def shapes(debug_str, list_of_variables):\n",
    "    return f\"{debug_str} {[v.shape for v in list_of_variables]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9681243f-0f72-47f7-a55d-3118185ab46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting debug\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def collage(pil_imgs, rows, cols):\n",
    "    n = len(pil_imgs) \n",
    "    if n != rows * cols:\n",
    "        raise Exception()\n",
    "    img_h, img_w = pil_imgs[0].size    \n",
    "    collage = Image.new('RGB', (rows*img_h, cols*img_w))\n",
    "    for i in range(n):\n",
    "        pr, pc = i%rows, i//rows\n",
    "        collage.paste(pil_imgs[i], (pr*img_h, pc*img_w))\n",
    "    return collage\n",
    "    \n",
    "def to_pil_img(a):\n",
    "    return Image.fromarray(np.array(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f22e10-db67-4783-aca1-d4b08a7ed85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.label_idx_to_str {0: '061', 1: '111', 2: '000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 15:34:20.759799: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version 12.6.77. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6, 64, 64, 3) tf.Tensor(\n",
      "[[0 0 2 2 1 1]\n",
      " [0 0 1 1 2 2]\n",
      " [1 1 0 0 2 2]\n",
      " [0 0 2 2 1 1]], shape=(4, 6), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "from data import ConstrastiveExamples\n",
    "\n",
    "# start with simple case of x1 R, G, B example\n",
    "c_egs = ConstrastiveExamples(\n",
    "    root_dir='data/reference_egs',\n",
    "    obj_ids=[\"061\", # \"135\",\"182\",  # x1 red\n",
    "             \"111\", # \"153\",\"198\",  # x1 green\n",
    "             \"000\", # \"017\",\"019\"   # x2 blue\n",
    "            ]\n",
    ")\n",
    "ds = c_egs.dataset(batch_size=opts.batch_size,\n",
    "                   objs_per_batch=opts.num_egs_per_class)\n",
    "for x, y in ds:\n",
    "    break\n",
    "\n",
    "x = jnp.array(x)\n",
    "print(x.shape, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54847fa9-847b-4ec0-a1e3-d631076b08c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x192>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall each element of outer batch is xN examples of anchor/positive pairs\n",
    "\n",
    "pil_imgs = list(map(to_pil_img, x[0]))\n",
    "collage(pil_imgs, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a4075-6a9f-4214-a97e-49f4d04312bc",
   "metadata": {},
   "source": [
    "model is simple enough embedding model\n",
    "\n",
    "output is L2 normalised embedding ( so dot products can be used for sims and xent contrastive )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e812617-4ac4-43fb-8b7d-e795703f8a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embeddings (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ l2_normalisation                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L2Normalisation</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embeddings (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ l2_normalisation                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mL2Normalisation\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,592</span> (416.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,592\u001b[0m (416.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,112</span> (414.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m106,112\u001b[0m (414.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">480</span> (1.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m480\u001b[0m (1.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def conv_bn_relu(filters, y):\n",
    "    y = Conv2D(filters=filters, strides=2, kernel_size=3, activation=None, padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    return Activation('relu')(y)\n",
    "\n",
    "class L2Normalisation(Layer):\n",
    "    def call(self, x):\n",
    "        norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n",
    "        return x / norm\n",
    "\n",
    "def construct_embedding_model():\n",
    "    input = Input((opts.height_width, opts.height_width, 3))\n",
    "    y = conv_bn_relu(filters=16, y=input)\n",
    "    y = conv_bn_relu(filters=32, y=y)\n",
    "    y = conv_bn_relu(filters=64, y=y)\n",
    "    y = conv_bn_relu(filters=128, y=y)\n",
    "    y = GlobalMaxPooling2D()(y)  # (B, E)\n",
    "\n",
    "    # embed, with normalisation\n",
    "    embeddings = Dense(\n",
    "        opts.embedding_dim,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=keras.initializers.TruncatedNormal(),\n",
    "        name='embeddings')(y)  # (B, E)\n",
    "    embeddings = L2Normalisation()(embeddings)\n",
    "    \n",
    "    return Model(input, embeddings)    \n",
    "\n",
    "embedding_model = construct_embedding_model()\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654991a8-79f3-42b4-9ad3-9cdeb14cab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# params, nt_params, [ntp.shape for ntp in nt_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a72a4a37-4c65-4162-96e3-dc239942102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0] (6, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((6, 64),\n",
       " Array([1.        , 0.99999994, 1.        , 1.0000001 , 1.        ,\n",
       "        1.        ], dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"x[0]\", x[0].shape)\n",
    "# embeddings = embedding_model(x[0])\n",
    "# embeddings.shape, jnp.linalg.norm(embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0a23c25-7179-4c24-b043-2b42b83ed9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e shape (6, 64)\n",
      "e norms [1.0000001  1.         1.0000001  1.         1.         0.99999994]\n",
      "ntps [(16,), (16,), (32,), (32,), (64,), (64,), (128,), (128,)]\n"
     ]
    }
   ],
   "source": [
    "# the model sees \"a batch\" as the set of (anchor, positive) pairs\n",
    "# whereas x is a batch of these.\n",
    "\n",
    "params = embedding_model.trainable_variables\n",
    "nt_params = embedding_model.non_trainable_variables\n",
    "\n",
    "embeddings, nt_params_2 = embedding_model.stateless_call(params, nt_params, x[0], training=True)\n",
    "\n",
    "print(\"e shape\", embeddings.shape)\n",
    "print(\"e norms\", jnp.linalg.norm(embeddings, axis=-1))\n",
    "print(shapes('ntps', nt_params_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72fe66e3-4460-438b-bbfd-abac504015c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 3.2589033\n",
      "ntps [(16,), (16,), (32,), (32,), (64,), (64,), (128,), (128,)]\n"
     ]
    }
   ],
   "source": [
    "# define the constrastive loss based on the 'batch' of 2N examples ( N pairs )\n",
    "\n",
    "def main_diagonal_softmax_cross_entropy(logits):\n",
    "    # cross entropy assuming \"labels\" are just (0, 1, 2, ...) i.e. where\n",
    "    # one_hot mask for log_softmax ends up just being the main diagonal\n",
    "    return -jnp.sum(jnp.diag(jax.nn.log_softmax(logits)))\n",
    "    \n",
    "def constrastive_loss(params, nt_params, x):\n",
    "    embeddings, nt_params = embedding_model.stateless_call(params, nt_params, x, training=True)\n",
    "    embeddings = embeddings.reshape((opts.num_egs_per_class, 2, opts.embedding_dim))\n",
    "    anchors = embeddings[:, 0]\n",
    "    positives = embeddings[:, 1]\n",
    "#    print('anchors', anchors.shape, 'positives', positives.shape)\n",
    "    gram_ish_matrix = jnp.einsum('ae,be->ab', anchors, positives)\n",
    "    xent = main_diagonal_softmax_cross_entropy(logits=gram_ish_matrix)\n",
    "    return jnp.mean(xent), nt_params\n",
    "\n",
    "loss, nt_params_2 = constrastive_loss(params, nt_params, x[0])\n",
    "print('loss', loss)\n",
    "print(shapes('ntps', nt_params_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43b6af92-0972-4097-b134-77f2c030f0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.288289\n",
      "1.1072654\n",
      "1.1069496\n",
      "1.1069436\n",
      "1.1069434\n",
      "1.1069435\n",
      "1.1069435\n",
      "1.1069435\n",
      "1.1069435\n",
      "1.1069435\n"
     ]
    }
   ],
   "source": [
    "# define gradients and a simple training loop\n",
    "\n",
    "def calculate_gradients(params, nt_params, x):\n",
    "    # x (2C,H,W,3)\n",
    "    grad_fn = value_and_grad(constrastive_loss, has_aux=True)    \n",
    "    (loss, nt_params), grads = grad_fn(params, nt_params, x)\n",
    "    return (loss, nt_params), grads\n",
    "\n",
    "opt = optax.adam(learning_rate=opts.learning_rate)\n",
    "\n",
    "def train_step(params, nt_params, opt_state, x):\n",
    "    (loss, nt_params), grads = calculate_gradients(params, nt_params, x)\n",
    "    updates, opt_state = opt.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, nt_params, opt_state, loss\n",
    "\n",
    "embedding_model = construct_embedding_model()\n",
    "\n",
    "params = embedding_model.trainable_variables\n",
    "nt_params = embedding_model.non_trainable_variables\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "for e in range(1000):\n",
    "    params, nt_params, opt_state, loss = jit(train_step)(params, nt_params, opt_state, x[0])\n",
    "    if e % 100 == 0:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cea3b7c9-8bbe-4c88-9c23-b001f4561e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1. ,  1. , -0.5, -0.5, -0.5, -0.5],\n",
       "       [ 1. ,  1. , -0.5, -0.5, -0.5, -0.5],\n",
       "       [-0.5, -0.5,  1. ,  1. , -0.5, -0.5],\n",
       "       [-0.5, -0.5,  1. ,  1. , -0.5, -0.5],\n",
       "       [-0.5, -0.5, -0.5, -0.5,  1. ,  1. ],\n",
       "       [-0.5, -0.5, -0.5, -0.5,  1. ,  1. ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test against batch\n",
    "\n",
    "embeddings, _ = embedding_model.stateless_call(params, nt_params, x[0], training=False)\n",
    "embeddings.shape\n",
    "\n",
    "# looks good (0,1) (2,3) (4,5) all pair well ( and others are -0.5 )\n",
    "\n",
    "jnp.around(jnp.dot(embeddings, embeddings.T), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a27837-0934-4aad-b986-8c94fc3c2b31",
   "metadata": {},
   "source": [
    "next we get things working on a batch of these examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09f3d449-ffae-4ce6-918f-62c30c15a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 15:36:32.040850: E external/xla/xla/service/hlo_lexer.cc:438] Failed to parse int literal: 62981654126569403251960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e shape (4, 6, 64)\n",
      "e norms [[1.         0.99999994 1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.        ]\n",
      " [0.99999994 1.         1.         1.         1.         1.        ]\n",
      " [1.         1.         1.0000001  1.         1.0000001  1.        ]]\n",
      "ntps [(4, 16), (4, 16), (4, 32), (4, 32), (4, 64), (4, 64), (4, 128), (4, 128)]\n"
     ]
    }
   ],
   "source": [
    "# to use the model batched we vmap it first \n",
    "\n",
    "def training_call(x):\n",
    "    return embedding_model.stateless_call(params, nt_params, x, training=True)\n",
    "\n",
    "training_call = vmap(training_call)\n",
    "embeddings, nt_params_2 = training_call(x)\n",
    "\n",
    "print(\"e shape\", embeddings.shape)\n",
    "print(\"e norms\", jnp.linalg.norm(embeddings, axis=-1))\n",
    "print(shapes('ntps', nt_params_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddc14ef6-32bd-4cea-9299-e5b91b6634ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntps [(), (), (), (), (), (), (), ()]\n"
     ]
    }
   ],
   "source": [
    "# but note that the nt_params returned have been vectorised too\n",
    "# i.e. they are [(B, p1), (B, p2), ...] instead of [(p1,), (p2,), ...]\n",
    "# so, we need to aggreate them,\n",
    "\n",
    "nt_params_2 = [jnp.mean(p, axis=0) for p in nt_params_2]\n",
    "print(shapes('ntps', nt_params_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02de11b1-7182-407c-a6e4-e6c5f8e0c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before inner batch aggregation...\n",
      "loss_v [1.1069435 2.8647838 3.1776733 2.8065224]\n",
      "ntps [(4, 16), (4, 16), (4, 32), (4, 32), (4, 64), (4, 64), (4, 128), (4, 128)]\n",
      "after inner batch aggregation...\n",
      "loss 2.4889808\n",
      "ntps [(16,), (16,), (32,), (32,), (64,), (64,), (128,), (128,)]\n"
     ]
    }
   ],
   "source": [
    "# as before we can vectorise the loss \n",
    "# takes (B, 2C, H, W, 3)\n",
    "constrastive_loss_v = vmap(constrastive_loss, in_axes=[None, None, 0])\n",
    "\n",
    "# and run over all of x \n",
    "loss_v, nt_params_2 = constrastive_loss_v(params, nt_params, x)  # (N)\n",
    "\n",
    "print(\"before inner batch aggregation...\")\n",
    "print('loss_v', loss_v)\n",
    "print(shapes('ntps', nt_params_2))\n",
    "\n",
    "loss = jnp.mean(loss_v)\n",
    "nt_params_2 = [jnp.mean(p, axis=0) for p in nt_params_2]\n",
    "\n",
    "print(\"after inner batch aggregation...\")\n",
    "print('loss', loss)\n",
    "print(shapes('ntps', nt_params_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "878cbfc8-7ebd-4dbb-beb8-92ab8b7031fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.4890444\n",
      "grads [(3, 3, 3, 16), (16,), (16,), (16,), (3, 3, 16, 32), (32,), (32,), (32,), (3, 3, 32, 64), (64,), (64,), (64,), (3, 3, 64, 128), (128,), (128,), (128,), (128, 64)]\n",
      "ntps [(16,), (16,), (32,), (32,), (64,), (64,), (128,), (128,)]\n"
     ]
    }
   ],
   "source": [
    "# we can calculate grads just as before but we must call a function that includes the loss \n",
    "# aggregation ( i.e. grads only are applicable for a scalar loss ) so wrap the vmap\n",
    "# and jnp.mean in one function.\n",
    "\n",
    "def constrastive_loss_v(params, nt_params, x):\n",
    "    # vectorise function as normal\n",
    "    loss_fn_v = vmap(constrastive_loss, in_axes=[None, None, 0])\n",
    "    # call returning vectorised result\n",
    "    loss_v, nt_params_v = loss_fn_v(params, nt_params, x)\n",
    "    # aggregate mean over both loss and nt_params for return\n",
    "    # TODO: what does this do for rng seeds?\n",
    "    loss = jnp.mean(loss_v)\n",
    "    nt_params = [jnp.mean(p, axis=0) for p in nt_params_v]    \n",
    "    return loss, nt_params\n",
    "\n",
    "(loss, nt_params_2), grads = jit(value_and_grad(constrastive_loss_v, has_aux=True))(params, nt_params, x)\n",
    "\n",
    "print('loss', loss)\n",
    "print(shapes('grads', grads))\n",
    "print(shapes('ntps', nt_params_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88b91293-e34f-4341-812f-3d2368fd0640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0 loss 3.1947274\n",
      "e 20 loss 1.7145796\n",
      "e 40 loss 1.1648873\n",
      "e 60 loss 1.1207416\n",
      "e 80 loss 1.1128668\n",
      "e 100 loss 1.1100806\n",
      "e 120 loss 1.108851\n",
      "e 140 loss 1.1081803\n",
      "e 160 loss 1.1077766\n",
      "e 180 loss 1.1075202\n"
     ]
    }
   ],
   "source": [
    "# stitch together into a training loop\n",
    "\n",
    "def main_diagonal_softmax_cross_entropy(logits):\n",
    "    # cross entropy assuming \"labels\" are just (0, 1, 2, ...) i.e. where\n",
    "    # one_hot mask for log_softmax ends up just being the main diagonal\n",
    "    return -jnp.sum(jnp.diag(jax.nn.log_softmax(logits)))\n",
    "    \n",
    "def constrastive_loss(params, nt_params, x):\n",
    "    # x (2C,H,W,3)\n",
    "    embeddings, nt_params = embedding_model.stateless_call(params, nt_params, x, training=True)\n",
    "    embeddings = embeddings.reshape((opts.num_egs_per_class, 2, opts.embedding_dim))\n",
    "    anchors = embeddings[:, 0]\n",
    "    positives = embeddings[:, 1]\n",
    "    gram_ish_matrix = jnp.einsum('ae,be->ab', anchors, positives)\n",
    "    xent = main_diagonal_softmax_cross_entropy(logits=gram_ish_matrix)\n",
    "    return jnp.mean(xent), nt_params\n",
    "\n",
    "def constrastive_loss_v(params, nt_params, x):\n",
    "    # x (B,2C,H,W,3)\n",
    "    loss_fn_v = vmap(constrastive_loss, in_axes=[None, None, 0])\n",
    "    loss_v, nt_params_v = loss_fn_v(params, nt_params, x)\n",
    "    loss = jnp.mean(loss_v)\n",
    "    return loss, nt_params_v\n",
    "\n",
    "def calculate_gradients(params, nt_params, x):\n",
    "    # x (B,2C,H,W,3)\n",
    "    grad_fn = value_and_grad(constrastive_loss_v, has_aux=True)    \n",
    "    (loss, nt_params_v), grads = grad_fn(params, nt_params, x)\n",
    "    return (loss, nt_params_v), grads\n",
    "\n",
    "opt = optax.adam(learning_rate=opts.learning_rate)\n",
    "\n",
    "def train_step(params, nt_params, opt_state, x):\n",
    "    (loss, nt_params_v), grads = calculate_gradients(params, nt_params, x)\n",
    "    updates, opt_state = opt.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    nt_params = [jnp.mean(p, axis=0) for p in nt_params_v]\n",
    "    return params, nt_params, opt_state, loss\n",
    "\n",
    "embedding_model = construct_embedding_model()\n",
    "\n",
    "params = embedding_model.trainable_variables\n",
    "nt_params = embedding_model.non_trainable_variables\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "for epoch in range(200):\n",
    "    params, nt_params, opt_state, loss = jit(train_step)(params, nt_params, opt_state, x)\n",
    "    if epoch % 20 == 0:\n",
    "        print('e', epoch, 'loss', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4611475-2611-427d-8661-41eb7dbef8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 0\n",
      "[[ 1.          0.96       -0.38       -0.41       -0.12       -0.17999999]\n",
      " [ 0.96        1.         -0.38       -0.39       -0.14999999 -0.19999999]\n",
      " [-0.38       -0.38        1.          0.96999997 -0.63       -0.61      ]\n",
      " [-0.41       -0.39        0.96999997  1.         -0.65       -0.59999996]\n",
      " [-0.12       -0.14999999 -0.63       -0.65        1.          0.91999996]\n",
      " [-0.17999999 -0.19999999 -0.61       -0.59999996  0.91999996  1.        ]]\n",
      "---------- 1\n",
      "[[ 1.          0.96       -0.38       -0.41       -0.12       -0.17999999]\n",
      " [ 0.96        1.         -0.38       -0.39       -0.14999999 -0.19999999]\n",
      " [-0.38       -0.38        1.          0.96999997 -0.63       -0.61      ]\n",
      " [-0.41       -0.39        0.96999997  1.         -0.65       -0.59999996]\n",
      " [-0.12       -0.14999999 -0.63       -0.65        1.          0.91999996]\n",
      " [-0.17999999 -0.19999999 -0.61       -0.59999996  0.91999996  1.        ]]\n",
      "---------- 2\n",
      "[[ 1.          0.96       -0.38       -0.41       -0.12       -0.17999999]\n",
      " [ 0.96        1.         -0.38       -0.39       -0.14999999 -0.19999999]\n",
      " [-0.38       -0.38        1.          0.96999997 -0.63       -0.61      ]\n",
      " [-0.41       -0.39        0.96999997  1.         -0.65       -0.59999996]\n",
      " [-0.12       -0.14999999 -0.63       -0.65        1.          0.91999996]\n",
      " [-0.17999999 -0.19999999 -0.61       -0.59999996  0.91999996  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# try a couple of the examples from the batch\n",
    "# each looks good\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"-\"*10, i)\n",
    "    embeddings, _ = embedding_model.stateless_call(params, nt_params, x[1], training=False)\n",
    "    print(jnp.around(jnp.dot(embeddings, embeddings.T), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1206be60-f6e4-45fe-9302-7afce153c5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how do some example look across batches though?\n",
    "\n",
    "x_ = jnp.stack([\n",
    "    x[0][0], \n",
    "    x[0][1],\n",
    "    x[1][4],\n",
    "    x[1][5]\n",
    "])\n",
    "\n",
    "pil_imgs = list(map(to_pil_img, x_))\n",
    "collage(pil_imgs, 2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0860d5ef-e506-4985-8bd9-003c9a14590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.96999997 0.94       0.93      ]\n",
      " [0.96999997 1.         0.90999997 0.91999996]\n",
      " [0.94       0.90999997 1.         0.96999997]\n",
      " [0.93       0.91999996 0.96999997 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# pretty good! the off diagonals are > 0.9\n",
    "# so it's generalising across the batch\n",
    "embeddings, _ = embedding_model.stateless_call(params, nt_params, x_, training=False)\n",
    "print(jnp.around(jnp.dot(embeddings, embeddings.T), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
