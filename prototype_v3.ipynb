{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc272dc-fa4f-4ffe-8c5e-18b5964ceb73",
   "metadata": {},
   "source": [
    "# v3 embedding model\n",
    "\n",
    "* includes two generators, ContrastiveExamples & SceneExamples for two inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79017548-62c2-4e27-85cc-a68fd8a25ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'jax'\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c87a4-5e25-414f-b3b4-2be157a651f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, jit, value_and_grad, nn\n",
    "\n",
    "import optax\n",
    "\n",
    "from data import ObjIdsHelper, ContrastiveExamples, SceneExamples\n",
    "from models.models import construct_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89020a1c-f22c-434d-97b0-3781301f9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opts:\n",
    "    num_batches = 1000         # effective epoch length\n",
    "    \n",
    "    obj_height_width = 64    \n",
    "    num_obj_references = 5     # N number of reference examples given for each object\n",
    "    num_focus_objs = 3         # C total number of classes used for contrasting & focus in scene\n",
    "    obj_filter_sizes = [8, 16, 32, 64]  \n",
    "    obj_embedding_dim = 64     # E dim for obj reference embeddings\n",
    "    \n",
    "    scene_height_width = 640    \n",
    "    scene_filter_sizes = [8, 16, 32, 64]\n",
    "    scene_feature_dim = 64     # F dim for scene features\n",
    "\n",
    "    classifier_filter_sizes = [16, 16]\n",
    "    \n",
    "    learning_rate = 1e-4\n",
    "\n",
    "opts = Opts()\n",
    "\n",
    "def shapes(debug_str, list_of_variables):\n",
    "    return f\"{debug_str} ({len(list_of_variables)}) {[v.shape for v in list_of_variables]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83b27e-6f08-4e21-bf78-435f6745c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.models import conv_bn_relu\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, GlobalMaxPooling2D, Reshape\n",
    "from keras.layers import Layer, BatchNormalization, Activation, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.initializers import TruncatedNormal, Constant\n",
    "\n",
    "def conv_bn_relu(filters, y, name, one_by_one=False):   \n",
    "    if one_by_one:\n",
    "        main = Conv2D(\n",
    "            filters=filters, strides=1, kernel_size=1,\n",
    "            activation=None, padding='same',\n",
    "            name=f\"{name}_conv1x1\")(y)\n",
    "    else:\n",
    "        main = Conv2D(\n",
    "            filters=filters, strides=2, kernel_size=3,\n",
    "            activation=None, padding='same',\n",
    "            name=f\"{name}_conv\")(y)\n",
    "        \n",
    "    main = BatchNormalization(name=f\"{name}_bn\")(main)\n",
    "    main = Activation('relu', name=f\"{name}_relu\")(main)\n",
    "\n",
    "    # TODO add this residual back in at end when scaling up\n",
    "    # branch = Conv2D(\n",
    "    #     filters=filters, strides=1, kernel_size=3,\n",
    "    #     activation=None, padding='same')(main)\n",
    "    # branch = BatchNormalization()(branch)\n",
    "    # branch = Activation('relu')(branch)\n",
    "    \n",
    "    return main #+ branch\n",
    "\n",
    "class L2Normalisation(Layer):\n",
    "    def call(self, x):\n",
    "        norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n",
    "        return x / norm\n",
    "\n",
    "class Tiling(Layer):    \n",
    "    def __init__(self, grid_size, name):\n",
    "        super().__init__(name=name)\n",
    "        self.grid_size = grid_size        \n",
    "    def call(self, x):\n",
    "        return jnp.tile(\n",
    "            x[:,None,None,:],\n",
    "            (1, self.grid_size, self.grid_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d826bf8-abcb-4668-aa87-7b3b716a27e3",
   "metadata": {},
   "source": [
    "## embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcda78a-4de1-4f18-9b45-df7849ddb7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_embedding_model(\n",
    "        height_width: int,\n",
    "        filter_sizes: List[int],\n",
    "        embedding_dim: int\n",
    "        ):\n",
    "\n",
    "    input = Input((height_width, height_width, 3))\n",
    "\n",
    "    y = input\n",
    "    for i, f in enumerate(filter_sizes):\n",
    "        y = conv_bn_relu(filters=f, y=y, name=f\"obj_e_{i}\")\n",
    "    y = GlobalMaxPooling2D(name='obj_e_gp')(y)  # (B, E)\n",
    "\n",
    "    # embed, with normalisation\n",
    "    embeddings = Dense(\n",
    "        embedding_dim,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=TruncatedNormal(),\n",
    "        name='obj_embeddings')(y)  # (B, E)\n",
    "    embeddings = L2Normalisation(name='obj_e_l2')(embeddings)\n",
    "\n",
    "    return Model(input, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015ddf1-ba34-4b51-bec3-5315307888c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = construct_embedding_model(\n",
    "    opts.obj_height_width, opts.obj_filter_sizes, opts.obj_embedding_dim)\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c0153-9906-4cbc-841f-7add25f69493",
   "metadata": {},
   "source": [
    "## scene model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87a5fc-acc1-4dc3-a138-acd19e0e342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_scene_model(\n",
    "    scene_height_width: int,\n",
    "    scene_filter_sizes: List[int],\n",
    "    scene_feature_dim: int,\n",
    "    expected_obj_embedding_dim: int,\n",
    "    classifier_filter_sizes: List[int]\n",
    "    ):\n",
    "\n",
    "    # scene backbone\n",
    "    scene_input = Input((scene_height_width, scene_height_width, 3), name='scene_input')\n",
    "    y = scene_input\n",
    "    for i, f in enumerate(scene_filter_sizes):\n",
    "        y = conv_bn_relu(filters=f, y=y, name=f\"scene_{i}\")\n",
    "        \n",
    "    # final feature layer ( projection, no relu )\n",
    "    scene_features = Dense(\n",
    "        scene_feature_dim,\n",
    "        use_bias=False, activation=None,\n",
    "        kernel_initializer=TruncatedNormal(),        \n",
    "        name='scene_features')(y)  # (B, F)    \n",
    "\n",
    "    # input branch from obj_embeddings\n",
    "    obj_embedding_input = Input((expected_obj_embedding_dim,), name='obj_embedding_inp')\n",
    "\n",
    "    # tile the embeddings to match the spatial size of the features \n",
    "    # from the scene backbone\n",
    "    grid_size = scene_features.shape[-2]  # assume square, dangerous?        \n",
    "    tiled_obj_embeddings = Tiling(grid_size, name='tiled_obj_emb')(obj_embedding_input)\n",
    "\n",
    "    # combine the two sets of features\n",
    "    obj_scene_features = Concatenate(axis=-1)([scene_features, tiled_obj_embeddings])\n",
    "\n",
    "    # add classifier ( logits )\n",
    "    classifier = obj_scene_features\n",
    "    for i, f in enumerate(scene_filter_sizes):\n",
    "        classifier = conv_bn_relu(filters=f, y=classifier, \n",
    "                                  name=f\"classifier_{i}\", one_by_one=True)\n",
    "    classifier = Dense(1, name='classifier')(classifier)\n",
    "\n",
    "    return Model(inputs=[scene_input, obj_embedding_input], \n",
    "                 outputs=classifier)\n",
    "\n",
    "scene_model = construct_scene_model(\n",
    "    scene_height_width=opts.scene_height_width,\n",
    "    scene_filter_sizes=[4, 8, 16, 32, 64, 64], #opts.scene_filter_sizes,\n",
    "    scene_feature_dim=opts.scene_feature_dim,\n",
    "    expected_obj_embedding_dim=opts.obj_embedding_dim,\n",
    "    classifier_filter_sizes=[8, 16] #opts.classifier_filter_sizes\n",
    ")\n",
    "scene_model.summary()\n",
    "\n",
    "# run example stateless\n",
    "B = 3\n",
    "eg_embedding_batch = np.ones((B, opts.obj_embedding_dim))\n",
    "eg_scene_batch = np.ones((B, opts.scene_height_width, opts.scene_height_width, 3))\n",
    "classifier_out, s_nt_params = scene_model.stateless_call(\n",
    "    scene_model.trainable_variables,\n",
    "    scene_model.non_trainable_variables,\n",
    "    [eg_scene_batch, eg_embedding_batch])\n",
    "print('classifier_out', classifier_out.shape)\n",
    "print(shapes('s_nt_params', s_nt_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c628f714-2a41-469c-92a3-b9dd758c8c84",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c79d7c4-7f34-4e37-9363-b78e83985c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "obj_ids_helper = ObjIdsHelper(\n",
    "    root_dir='data/train/reference_patches/',\n",
    "    obj_ids=[\"061\", \"135\",\"182\",  # x3 red\n",
    "             \"111\", \"153\",\"198\",  # x3 green\n",
    "             \"000\", \"017\",\"019\"], # x3 blue\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "obj_egs = ContrastiveExamples(obj_ids_helper)\n",
    "obj_ds = obj_egs.dataset(num_batches=opts.num_batches,\n",
    "                   num_obj_references=opts.num_obj_references,\n",
    "                   num_contrastive_examples=opts.num_focus_objs)\n",
    "\n",
    "scene_egs = SceneExamples(\n",
    "    obj_ids_helper=obj_ids_helper,\n",
    "    grid_size=10,\n",
    "    num_other_objs=4,\n",
    "    instances_per_obj=3,\n",
    "    seed=123)\n",
    "scene_ds = scene_egs.dataset(\n",
    "    num_batches=opts.num_batches,\n",
    "    num_focus_objects=opts.num_focus_objs)\n",
    "\n",
    "for (obj_x, _obj_y), (scene_x, scene_y_true) in zip(obj_ds, scene_ds):\n",
    "    obj_x = jnp.array(obj_x)    \n",
    "    scene_x = jnp.array(scene_x)\n",
    "    scene_y_true = jnp.array(scene_y_true)    \n",
    "    print('obj_ x', obj_x.shape)\n",
    "    print('scene_ x', scene_x.shape, 'y_true', scene_y_true.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c615fb2d-0924-4162-8e4d-a73ce846c301",
   "metadata": {},
   "source": [
    "## composite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab372e4-e952-46e3-930b-b5608b5af9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = { 'constrastive': 1.0, 'scene': 100.0 }\n",
    "\n",
    "def mean_embeddings(params, nt_params, x, training):\n",
    "    # x (N, H, W, 3)\n",
    "    embeddings, nt_params = embedding_model.stateless_call(\n",
    "        params, nt_params, x, training=training)  # (N, E)\n",
    "    # average over N\n",
    "    embeddings = jnp.mean(embeddings, axis=0)  # (E)\n",
    "    # (re) L2 normalise\n",
    "    embeddings /= jnp.linalg.norm(embeddings, axis=-1, keepdims=True)\n",
    "    return embeddings, nt_params  # (E,)\n",
    "\n",
    "def main_diagonal_softmax_cross_entropy(logits):\n",
    "    # cross entropy assuming \"labels\" are just (0, 1, 2, ...) i.e. where\n",
    "    # one_hot mask for log_softmax ends up just being the main diagonal\n",
    "    return -jnp.sum(jnp.diag(nn.log_softmax(logits)))\n",
    "\n",
    "def forward(params, nt_params, obj_x, scene_x, training):\n",
    "    # obj_x    (C, 2, N, oHW, oHW, 3)\n",
    "    # scene_x  (C, sHW, sHW, 3)\n",
    "    # scene_y  (C, G, G, 1)\n",
    "    \n",
    "    e_params, s_params = params\n",
    "    e_nt_params, s_nt_params = nt_params\n",
    "\n",
    "    # first run obj reference branch\n",
    "    \n",
    "    # first flatten obj_x to single 2C \"batch\" over N to get common batch norm stats\n",
    "    # TODO: how are these stats skewed w.r.t to fact we'll call over N during inference\n",
    "    C = obj_x.shape[0]\n",
    "    nhwc = obj_x.shape[-4:]\n",
    "    obj_x = obj_x.reshape((-1, *nhwc))  # (2C, N, oHW, oHW, 3)\n",
    "\n",
    "    # run through mean embeddings which reduces over N\n",
    "    # ( and average non trainables )\n",
    "    v_mean_embeddings = vmap(mean_embeddings, in_axes=(None, None, 0, None))\n",
    "    obj_embeddings, e_nt_params = v_mean_embeddings(\n",
    "        e_params, e_nt_params, obj_x, training)  # (2C, E)\n",
    "    e_nt_params = [jnp.mean(p, axis=0) for p in e_nt_params]\n",
    "\n",
    "    # reshape back to split anchors and positives\n",
    "    obj_embeddings = obj_embeddings.reshape((C, 2, -1))  # (C, 2, E)\n",
    "    anchors = obj_embeddings[:,0]\n",
    "    positives = obj_embeddings[:,1]\n",
    "    #print('anchors', anchors.shape)\n",
    "\n",
    "    # second; run scene branch runs ( with just anchors for obj references )\n",
    "    \n",
    "    # classifier_out (C, G, G, 1) ( logits )\n",
    "    classifier_out, s_nt_params = scene_model.stateless_call(\n",
    "        s_params, s_nt_params, [scene_x, anchors], training=training)\n",
    "    #print('classifier_out', classifier_out.shape)\n",
    "    #print(shapes('s_nt_params', s_nt_params))\n",
    "\n",
    "    nt_params = e_nt_params, s_nt_params\n",
    "    return anchors, positives, classifier_out, nt_params\n",
    "\n",
    "def calculate_individual_losses(params, nt_params, obj_x, scene_x, scene_y_true):\n",
    "    # obj_x    (C, 2, N, oHW, oHW, 3)\n",
    "    # scene_x  (C, sHW, sHW, 3)\n",
    "    # scene_y  (C, G, G, 1)\n",
    "\n",
    "    # run forward through two networks\n",
    "    anchors, positives, classifier_out, nt_params = forward(\n",
    "        params, nt_params, obj_x, scene_x, training=True)\n",
    "        \n",
    "    # calculate contrastive loss from obj embeddings\n",
    "    gram_ish_matrix = jnp.einsum('ae,be->ab', anchors, positives)\n",
    "    metric_losses = main_diagonal_softmax_cross_entropy(logits=gram_ish_matrix)\n",
    "    metric_loss = jnp.mean(metric_losses)\n",
    "\n",
    "    # calculate classifier loss is binary cross entropy ( mean across all instances )\n",
    "    scene_losses = optax.losses.sigmoid_binary_cross_entropy(\n",
    "        logits=classifier_out.flatten(),\n",
    "        labels=scene_y_true.flatten())\n",
    "    scene_loss = jnp.mean(scene_losses)\n",
    "\n",
    "    # return losses ( with nt_params updated from forward call )\n",
    "    return metric_loss, scene_loss, nt_params\n",
    "\n",
    "def calculate_single_loss(params, nt_params, obj_x, scene_x, scene_y_true):\n",
    "    metric_loss, scene_loss, nt_params = calculate_individual_losses(\n",
    "        params, nt_params, obj_x, scene_x, scene_y_true)\n",
    "    loss = (loss_weights['constrastive']) * metric_loss + (loss_weights['scene'] * scene_loss)\n",
    "    return loss,  nt_params\n",
    "\n",
    "def calculate_gradients(params, nt_params, obj_x, scene_x, scene_y_true):\n",
    "    # obj_x    (C, 2, N, oHW, oHW, 3)\n",
    "    # scene_x  (C, sHW, sHW, 3)\n",
    "    # scene_y  (C, G, G, 1)\n",
    "    grad_fn = value_and_grad(calculate_single_loss, has_aux=True)    \n",
    "    (loss, nt_params), grads = grad_fn(\n",
    "        params, nt_params, obj_x, scene_x, scene_y_true)\n",
    "    return (loss, nt_params), grads\n",
    "\n",
    "opt = optax.adam(learning_rate=opts.learning_rate)\n",
    "\n",
    "@jit\n",
    "def train_step(\n",
    "    params, nt_params, opt_state, \n",
    "    obj_x, scene_x, scene_y_true):\n",
    "\n",
    "    # calculate gradients\n",
    "    (loss, nt_params), grads = calculate_gradients(\n",
    "        params, nt_params, obj_x, scene_x, scene_y_true)\n",
    "    \n",
    "    # # this is bit clumsy; because params was passed to grad call \n",
    "    # # with _all_ params (including non trainables, we get back \n",
    "    # # grads w.r.t to the non trainables ( which\n",
    "    # # will be zero and can be ignored... )\n",
    "    # e_params, _, s_params, _ = params\n",
    "    # e_grads, _, s_grads, _ = grads\n",
    "\n",
    "    # calculate updates from optimiser\n",
    "    updates, opt_state = opt.update(grads, opt_state, params)\n",
    "\n",
    "    # apply updates to get new params\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    # return\n",
    "    return params, nt_params, opt_state, loss\n",
    "   \n",
    "\n",
    "e_params = embedding_model.trainable_variables\n",
    "e_nt_params = embedding_model.non_trainable_variables\n",
    "s_params = scene_model.trainable_variables\n",
    "s_nt_params = scene_model.non_trainable_variables\n",
    "# print(shapes('e_params', e_params))\n",
    "# print(shapes('e_nt_params', e_nt_params))\n",
    "# print(shapes('s_params', s_params))\n",
    "# print(shapes('s_nt_params', s_nt_params))\n",
    "\n",
    "# package up trainable and non trainables in tuples\n",
    "params = e_params, s_params\n",
    "nt_params = e_nt_params, s_nt_params\n",
    "\n",
    "# optimser will run against both\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "for e, ((obj_x, _obj_y), (scene_x, scene_y_true)) in enumerate(zip(obj_ds, scene_ds)):\n",
    "    obj_x = jnp.array(obj_x)    \n",
    "    scene_x = jnp.array(scene_x)\n",
    "    scene_y_true = jnp.array(scene_y_true)   \n",
    "    \n",
    "    params, nt_params, opt_state, loss = train_step(\n",
    "        params, nt_params, opt_state,\n",
    "        obj_x, scene_x, scene_y_true)\n",
    "    \n",
    "    if e % 50 == 0:\n",
    "        metric_loss, scene_loss, _ = calculate_individual_losses(\n",
    "            params, nt_params, obj_x, scene_x, scene_y_true)\n",
    "        print('metric_loss', metric_loss, 'scene_loss', scene_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a88fe-a0a7-49d4-af20-0638959f28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jit\n",
    "def test_step(\n",
    "    params, nt_params,\n",
    "    obj_x, scene_x):\n",
    "    _anchors, _positives, classifier_out, _nt_params = forward(\n",
    "        params, nt_params, obj_x, scene_x, training=False)\n",
    "    return classifier_out\n",
    "\n",
    "import jax\n",
    "\n",
    "y_pred = jax.nn.sigmoid(test_step(params, nt_params, obj_x, scene_x).squeeze())\n",
    "print(y_pred)\n",
    "\n",
    "y_pred = (y_pred > 0.1).astype(int)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81168e9-e7a4-4bc2-aeaa-042ed92f7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from util import to_pil_img, highlight, collage\n",
    "    \n",
    "    obj_x.shape, scene_x.shape\n",
    "    \n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c35ed-acd3-47cf-974e-398a24ffa525",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors0 = obj_x[0, 0]\n",
    "anchors0 = list(map(to_pil_img, anchors0))\n",
    "collage(anchors0, 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11ce68-d289-4a5f-8173-811cefb3c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene0 = to_pil_img(scene_x[0])\n",
    "scene0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8a0c3-ac46-4251-9e3e-29b0c16d5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight(scene0, y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5e1e7-3c1c-43a5-9e19-d9637c1516a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
